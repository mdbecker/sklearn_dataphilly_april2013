{
 "metadata": {
  "name": "06_validation_and_testing"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Measuring Classification Performance: Validation & Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most common mistake beginners commit when training statistical models is to evaluate the quality of the model on the same data used for fitting the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the data\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instantiate and train the classifier\n",
      "from sklearn.svm import LinearSVC\n",
      "clf = LinearSVC(loss = 'l2')\n",
      "clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check input vs. output labels\n",
      "y_pred = clf.predict(X)\n",
      "print (y_pred == y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Question:** what might be the problem with this approach?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The overfitting issue"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem is that some models can be subject to overfitting: they can learn the training data by heart without generalizing. The symptoms are:\n",
      "\n",
      " * The predictive accurracy on the data used for training can be excellent (sometimes 100%)\n",
      " * The models do little better than random prediction when facing new data that was not part of the training set\n",
      "\n",
      "If you evaluate your model on your training data you won\u2019t be able to tell whether your model is overfitting or not."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Better Approach: Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Learning the parameters of a prediction function and testing it on the\n",
      "same data is a methodological mistake: a model that would just repeat\n",
      "the labels of the samples that it has just seen would have a perfect\n",
      "score but would fail to predict anything useful on yet-unseen data.\n",
      "\n",
      "To avoid over-fitting, we have to define two different sets:\n",
      "\n",
      "- a training set X_train, y_train which is used for learning the parameters of a predictive model\n",
      "- a testing set X_test, y_test which is used for evaluating the fitted predictive model\n",
      "\n",
      "In scikit-learn such a random split can be quickly computed with the\n",
      "`train_test_split` helper function.  It can be used this way:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
      "\n",
      "print X.shape, X_train.shape, X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we train on the training data, and test on the testing data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = LinearSVC(loss='l2').fit(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print (y_pred == y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is an issue here, however:\n",
      "by defining these two sets, we drastically reduce the number\n",
      "of samples which can be used for learning the model, and the results\n",
      "can depend on a particular random choice for the pair of (train, test) sets.\n",
      "\n",
      "A solution is to split the whole data several consecutive times in different\n",
      "train set and test set, and to return the averaged value of the prediction\n",
      "scores obtained with the different sets. Such a procedure is called **cross-validation**.\n",
      "This approach can be computationally expensive, but does not waste too much data\n",
      "(as it is the case when fixing an arbitrary test set), which is a major advantage\n",
      "in problem such as inverse inference where the number of samples is very small.\n",
      "\n",
      "We'll explore cross-validation a bit later, but\n",
      "you can find much more information on cross-validation in scikit-learn here:\n",
      "http://scikit-learn.org/dev/modules/cross_validation.html\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}